{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis sintáctico\n",
    "\n",
    "El **análisis sintáctico** es, en el campo de la lingüística, el análisis de las funciones sintácticas o relaciones de concordancia y jerarquía que guardan las palabras agrupándose entre sí en sintagmas u oraciones.\n",
    "\n",
    "Un analizador sintáctico (o **parser**) es un programa informático que analiza una cadena de símbolos de acuerdo a las reglas de una gramática formal. Usualmente hace parte de un compilador, en cuyo caso, transforma una entrada en un **árbol sintáctico de derivación**.\n",
    "\n",
    "El análisis sintáctico convierte el texto de entrada en otras estructuras (comúnmente árboles), que son más útiles para el posterior análisis y capturan la jerarquía implícita de la entrada. Un **analizador léxico** crea **tokens** de una secuencia de caracteres de entrada y son estos tokens los que son procesados por el analizador sintáctico para construir la estructura de datos, por ejemplo un árbol de análisis o árboles de sintaxis abstracta.\n",
    "\n",
    "**Lenguajes de programación**\n",
    "El uso más común de los analizadores sintácticos es como parte de la fase de análisis de los compiladores. De modo que tienen que analizar el código fuente del lenguaje. Los lenguajes de programación tienden a basarse en gramáticas libres de contexto, debido a que se pueden escribir analizadores rápidos y eficientes para estas.\n",
    "\n",
    "Las gramáticas libres de contexto tienen una expresividad limitada y sólo pueden expresar un conjunto limitado de lenguajes. Informalmente la razón de esto es que la memoria de un lenguaje de este tipo es limitada, la gramática no puede recordar la presencia de una construcción en una entrada arbitrariamente larga y esto es necesario en un lenguaje en el que por ejemplo una variable debe ser declarada antes de que pueda ser referenciada. Las gramáticas más complejas no pueden ser analizadas de forma eficiente. Por estas razones es común crear un analizador permisivo para una gramática libre de contexto que acepta un superconjunto del lenguaje (acepta algunas construcciones inválidas), después del análisis inicial las construcciones incorrectas pueden ser filtradas.\n",
    "\n",
    "Normalmente es fácil definir una gramática libre de contexto que acepte todas las construcciones de un lenguaje pero por el contrario es prácticamente imposible construir una gramática libre de contexto que admita solo las construcciones deseadas. En cualquier caso la mayoría de analizadores no son construidos a mano sino usando generadores automáticos.\n",
    "\n",
    "**Clasificación**\n",
    "La tarea esencial de un analizador es determinar si una determinada entrada puede ser derivada desde el símbolo inicial, usando las reglas de una gramática formal, y como hacer esto, existen esencialmente dos formas:\n",
    "\n",
    "* Analizador sintáctico descendente (Top-Down-Parser): ..un analizador puede empezar con el símbolo inicial e intentar transformarlo en la entrada, intuitivamente esto sería ir dividiendo la entrada progresivamente en partes cada vez más pequeñas, de esta forma funcionan los analizadores LL, un ejemplo es el javaCC. Una mejora en estos parsers se puede logar usando GLR (Generalized Left-to-right Rightmost derivation).\n",
    "\n",
    "* Analizador sintáctico ascendente (Bottom-Up-Parser): un analizador puede empezar con la entrada e intentar llegar hasta el símbolo inicial, intuitivamente el analizador intenta encontrar los símbolos más pequeños y progresivamente construir la jerarquía de símbolos hasta el inicial, los analizadores LR funcionan así y un ejemplo es el Yacc. También existen SLR (Simple LR) o los LALR (Look-ahead LR) como también de los GLL7​ (Generalized Left-to-right Leftmost derivation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analizar HTML con Python: HTMLParser\n",
    "El módulo **html.parse** define la clase **HTMLParser** la cual sirve de base para el análisis de archivos de texto con formato en HTML y XHTML. Cuando se vincula un archivo HTML a un objeto HTMLParser, este lo procesará de principio a fin, encontrando las etiquetas de apertura, etiquetas de cierre, datos de texto y otros componentes en el archivo fuente y “procesar” cada uno de estos elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "\n",
      "<head>\n",
      "\n",
      "\t<title>Primer PÃ¡gina</title>\n",
      "\n",
      "</head>\n",
      "\n",
      "<body>\n",
      "\n",
      "\t<h1> hola mundo </h1>\n",
      "\n",
      "</body>\n",
      "\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "archivo = open('holamundo.html',\"r\")\n",
    " \n",
    "for lineas in archivo.readlines() :\n",
    "    print(lineas)\n",
    " \n",
    "archivo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='holamundo.html' mode='r' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "with open('holamundo.html','r') as file:\n",
    "    parser = HTMLParser()\n",
    "    parser.feed(file.read())\n",
    "    \n",
    "print(file)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itcolima.edu.mx 73\n",
      "dspace.itcolima.edu.mx 1\n",
      "www.facebook.com 1\n",
      "sites.google.com 1\n",
      "citt.itsm.edu.mx 1\n",
      "www.universia.net.mx 1\n",
      "consultaplaneacion.tecnm.mx 1\n",
      "www.premiosantander.com 1\n",
      "twitter.com 1\n",
      "www.youtube.com 1\n",
      "conacytprensa.mx 1\n",
      "www.conricyt.mx 1\n"
     ]
    }
   ],
   "source": [
    "#Using the htmlparser with python find all the links included in the ITC page.\n",
    "from requests_html import HTMLSession\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "session = HTMLSession()\n",
    "r = session.get(\"https://itcolima.edu.mx/www/\")\n",
    "unique_netlocs = Counter(urlparse(link).netloc for link in r.html.absolute_links)\n",
    "for link in unique_netlocs:\n",
    "    print(link, unique_netlocs[link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests-html\n",
      "  Downloading https://files.pythonhosted.org/packages/24/bc/a4380f09bab3a776182578ce6b2771e57259d0d4dbce178205779abdc347/requests_html-0.10.0-py3-none-any.whl\n",
      "Requirement already satisfied: requests in c:\\users\\fer\\anaconda3\\lib\\site-packages (from requests-html) (2.21.0)\n",
      "Collecting parse (from requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/c4/c0/324d280a3298cdad806c3fb64eef31aed5c4dbd15b72a309498fb71c6a17/parse-1.12.0.tar.gz\n",
      "Collecting fake-useragent (from requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
      "Collecting pyquery (from requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/09/c7/ce8c9c37ab8ff8337faad3335c088d60bed4a35a4bed33a64f0e64fbcf29/pyquery-1.4.0-py2.py3-none-any.whl\n",
      "Collecting w3lib (from requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/81/43/9dcf92a77f5f0afe4f4df2407d7289eea01368a08b64bda00dd318ca62a6/w3lib-1.20.0-py2.py3-none-any.whl\n",
      "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/16/a5e8d617994cac605f972523bb25f12e3ff9c30baee29b4a9c50467229d9/pyppeteer-0.0.25.tar.gz (1.2MB)\n",
      "Collecting bs4 (from requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\fer\\anaconda3\\lib\\site-packages (from requests->requests-html) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\fer\\anaconda3\\lib\\site-packages (from requests->requests-html) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fer\\anaconda3\\lib\\site-packages (from requests->requests-html) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\fer\\anaconda3\\lib\\site-packages (from requests->requests-html) (2.8)\n",
      "Requirement already satisfied: lxml>=2.1 in c:\\users\\fer\\anaconda3\\lib\\site-packages (from pyquery->requests-html) (4.3.2)\n",
      "Collecting cssselect>0.7.9 (from pyquery->requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\fer\\anaconda3\\lib\\site-packages (from w3lib->requests-html) (1.12.0)\n",
      "Collecting pyee (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/d8/5608d571ffad3d7de0192b0b3099fe3f38d87c0817ebff3cee19264f0bc2/pyee-6.0.0-py2.py3-none-any.whl\n",
      "Collecting websockets (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/3b/d475aa1144dc8efc8c2caded6da5dc82ea9d4dc7c460d2dfe62ae4e98454/websockets-8.0-cp37-cp37m-win_amd64.whl (65kB)\n",
      "Collecting appdirs (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm in c:\\users\\fer\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (4.31.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\fer\\anaconda3\\lib\\site-packages (from bs4->requests-html) (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\fer\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4->requests-html) (1.8)\n",
      "Building wheels for collected packages: parse, fake-useragent, pyppeteer, bs4\n",
      "  Building wheel for parse (setup.py): started\n",
      "  Building wheel for parse (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\fer\\AppData\\Local\\pip\\Cache\\wheels\\9f\\62\\d1\\c46b7452aa0b2c838080bdd462110cd6c61890151f916aa743\n",
      "  Building wheel for fake-useragent (setup.py): started\n",
      "  Building wheel for fake-useragent (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\fer\\AppData\\Local\\pip\\Cache\\wheels\\5e\\63\\09\\d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
      "  Building wheel for pyppeteer (setup.py): started\n",
      "  Building wheel for pyppeteer (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\fer\\AppData\\Local\\pip\\Cache\\wheels\\34\\e0\\5d\\070e22eceecf7ecd5fa4b86bbc18c1c7d0b90e02e9b57f35eb\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\fer\\AppData\\Local\\pip\\Cache\\wheels\\a0\\b0\\b2\\4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built parse fake-useragent pyppeteer bs4\n",
      "Installing collected packages: parse, fake-useragent, cssselect, pyquery, w3lib, pyee, websockets, appdirs, pyppeteer, bs4, requests-html\n",
      "Successfully installed appdirs-1.4.3 bs4-0.0.1 cssselect-1.0.3 fake-useragent-0.1.11 parse-1.12.0 pyee-6.0.0 pyppeteer-0.0.25 pyquery-1.4.0 requests-html-0.10.0 w3lib-1.20.0 websockets-8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests-html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiqueta encontrada ... DOCTYPE html\n",
      "Encountered some data:  \n",
      "\n",
      "Encountered a start tag:  html\n",
      "Encountered some data:  \n",
      "\n",
      "Encountered a start tag:  head\n",
      "Encountered some data:  \n",
      "\t\n",
      "Encountered a start tag:  title\n",
      "Encountered some data:  Primer PÃ¡gina\n",
      "Encountered an end tag:  title\n",
      "Encountered some data:  \n",
      "\n",
      "Encountered an end tag:  head\n",
      "Encountered some data:  \n",
      "\n",
      "Encountered a start tag:  body\n",
      "Encountered some data:  \n",
      "\t\n",
      "Encountered a start tag:  h1\n",
      "Encountered some data:   hola mundo \n",
      "Encountered an end tag:  h1\n",
      "Encountered some data:  \n",
      "\n",
      "Encountered an end tag:  body\n",
      "Encountered some data:  \n",
      "\n",
      "Encountered an end tag:  html\n"
     ]
    }
   ],
   "source": [
    "# Script para identificar la etiqueta <!DOCTYPE html>\n",
    "# En un archivo html\n",
    "# \n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    def handle_decl(self,data):\n",
    "        print(\"Etiqueta encontrada ...\",data)\n",
    "        \n",
    "    def handle_starttag(self,tag,attrs):\n",
    "        print(\"Encountered a start tag: \",tag)\n",
    "    \n",
    "    def handle_endtag(self,tag):\n",
    "        print(\"Encountered an end tag: \",tag)\n",
    "    \n",
    "    def handle_data(self,data):\n",
    "        print(\"Encountered some data: \",data)\n",
    "        \n",
    "parser = MyHTMLParser()\n",
    "    \n",
    "with open(\"C:/Users/fer/Desktop/python/holamundo.html\",\"r\" )as f:\n",
    "    page=f.read()\n",
    "        \n",
    "parser.feed(str(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
